import os
import warnings
import torch
import whisper
import yt_dlp
from typing import Optional

warnings.filterwarnings("ignore")


def download_audio_from_youtube(url: str, output_path="temp_audio") -> str | None:
    """ìœ íŠœë¸Œ ì˜ìƒì„ MP3ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    ydl_opts = {
        'format': 'bestaudio/best',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
        'outtmpl': output_path,
        'quiet': True,
    }
    print(f"ğŸ“¥ [1/4] ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘... ({url})")
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])
        return f"{output_path}.mp3"
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


def transcribe_with_local_whisper(audio_path: str, model_size="base") -> str | None:
    """ë¡œì»¬ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âš™ï¸ [2/4] STT ë³€í™˜ ì¤‘... (ì¥ì¹˜: {device}, ëª¨ë¸: {model_size})")
    try:
        model = whisper.load_model(model_size, device=device)
        result = model.transcribe(audio_path, fp16=(device == "cuda"))
        return result["text"]
    except Exception as e:
        print(f"âŒ STT ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None
