import os
import re
import json
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ETL ë‹¨ê³„ë³„ ëª¨ë“ˆ(ê°™ì€ ë””ë ‰í„°ë¦¬)
from extract import download_audio_from_youtube, transcribe_with_local_whisper, fetch_subtitles_from_youtube
from transform import extract_structured_data
from load import upload_to_qdrant


def _safe_name_from_url(url: str) -> str:
    name = re.sub(r"[^0-9a-zA-Z]+", "_", url)
    return name[:100]


def _read_url_list() -> list:
    # read url_list.txt from the same `data` directory as this script
    path = os.path.join(os.path.dirname(__file__), 'url_list.txt')
    if not os.path.exists(path):
        return []
    urls = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            s = line.strip()
            if not s or s.startswith('#'):
                continue
            urls.append(s)
    return urls


if __name__ == "__main__":
    urls = _read_url_list()
    if not urls:
        # Fallback to default list if url_list.txt is missing or empty
        urls = [
            "https://www.youtube.com/watch?v=F_LgyPSEYcY",
            "https://www.youtube.com/watch?v=EKAuoWFfn-s",
            "https://www.youtube.com/watch?v=kEpCKAAmUt8"
        ]
        print("Using default URL list.")

    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
    all_point_ids = []

    for url in urls:
        print(f"\n=== Processing: {url} ===")
        audio_file = None
        safe = _safe_name_from_url(url)
        raw_script = None

        try:
            # 1) try subtitles first
            raw_script = fetch_subtitles_from_youtube(url)
            if raw_script:
                print("ìë§‰ìœ¼ë¡œë¶€í„° í…ìŠ¤íŠ¸ í™•ë³´ â€” STT ë‹¨ê³„ ìŠ¤í‚µ")
            else:
                # 2) download audio and transcribe
                audio_file = download_audio_from_youtube(url, output_path=f"temp_audio_{safe}")
                if audio_file and os.path.exists(audio_file):
                    raw_script = transcribe_with_local_whisper(audio_file, model_size="base")
                
            if not raw_script:
                print(f"âŒ {url}ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨, ê±´ë„ˆëœ€")
                continue

            print(f"\n--- ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_script)} ì ---")

            # save raw script
            txt_path = os.path.join(repo_root, f"{safe}_raw_script.txt")
            try:
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(raw_script)
                print(f"ğŸ“„ Raw script saved to {txt_path}")
            except Exception as e:
                print(f"âš ï¸ Failed to save raw script: {e}")

            # 3) transform (LLM êµ¬ì¡°í™”)
            structured_data = extract_structured_data(raw_script)
            print(f"âœ… ì´ {len(structured_data.episodes)}ê°œì˜ ì—í”¼ì†Œë“œ ì¶”ì¶œë¨")

            # 4) load (ì ì¬)
            for episode in structured_data.episodes:
                point_id = upload_to_qdrant("love_counseling_db", episode)
                all_point_ids.append(point_id)

        except Exception as e:
            print(f"Error processing {url}: {e}")
        finally:
            if audio_file and os.path.exists(audio_file):
                os.remove(audio_file)

    # 5) ìƒì„±ëœ ëª¨ë“  Point IDë¥¼ íŒŒì¼ì— ì €ì¥
    if all_point_ids:
        ids_path = os.path.join(os.path.dirname(__file__), "point_ids.txt")
        try:
            with open(ids_path, "w", encoding="utf-8") as f:
                for pid in all_point_ids:
                    f.write(f"{pid}\n")
            print(f"\nğŸ“‚ ëª¨ë“  Point ID({len(all_point_ids)}ê°œ)ê°€ {ids_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ Point ID ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
