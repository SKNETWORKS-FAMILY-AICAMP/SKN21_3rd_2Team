
import os
import uuid
import json
import warnings
import torch
import whisper
import yt_dlp
from typing import List
from pydantic import BaseModel, Field
from openai import OpenAI
from qdrant_client import QdrantClient
from qdrant_client.models import PointStruct, VectorParams, Distance
from dotenv import load_dotenv

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
warnings.filterwarnings("ignore")

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv()

# API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
qdrant = QdrantClient(
    url=os.getenv("QDRANT_URL"),
    api_key=os.getenv("QDRANT_API_KEY") # í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œ í‚¤ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
)

# ==========================================
# [PART 1] ìœ íŠœë¸Œ ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ë° STT (ë¬´ë£Œ/ë¡œì»¬)
# ==========================================

def download_audio_from_youtube(url: str, output_path="temp_audio") -> str | None:
    # 1ë²ˆì—ì„œ ì„¤ì¹˜í•œ ì‹¤ì œ ê²½ë¡œë¥¼ ì§ì ‘ ì…ë ¥
    MY_FFMPEG_PATH = r"C:\ffmpeg\bin" 

    ydl_opts = {
        'format': 'bestaudio/best',
        # yt-dlpì—ê²Œ ì—”ì§„ ìœ„ì¹˜ë¥¼ ê°•ì œë¡œ ì•Œë ¤ì¤Œ
        'ffmpeg_location': MY_FFMPEG_PATH, 
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': 'mp3',
            'preferredquality': '192',
        }],
        'outtmpl': output_path,
        'quiet': True,
    }
    
    print(f"ğŸ“¥ [1/4] ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘... ({url})")
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])
        return f"{output_path}.mp3"
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
    print(f"ğŸ“¥ [1/4] ì˜¤ë””ì˜¤ ë‹¤ìš´ë¡œë“œ ì¤‘... ({url})")
    try:
        with yt_dlp.YoutubeDL(ydl_opts) as ydl:
            ydl.download([url])
        return f"{output_path}.mp3"
    except Exception as e:
        print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def transcribe_with_local_whisper(audio_path: str, model_size="base") -> str | None:
    """ë¡œì»¬ Whisper ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"âš™ï¸ [2/4] STT ë³€í™˜ ì¤‘... (ì¥ì¹˜: {device}, ëª¨ë¸: {model_size})")
    
    try:
        model = whisper.load_model(model_size, device=device)
        result = model.transcribe(audio_path, fp16=(device == "cuda"))
        return result["text"]
    except Exception as e:
        print(f"âŒ STT ë³€í™˜ ì‹¤íŒ¨: {e}")
        return None

# ==========================================
# [PART 2] ë°ì´í„° êµ¬ì¡° ì •ì˜ (Pydantic)
# ==========================================

class RetrievalMetadata(BaseModel):
    relationship_stage: str = Field(description="ì˜ˆ: ì¸, ì—°ì• ì´ˆê¸°, ê¶Œíƒœê¸°, ì´ë³„, ì¬íšŒ ë“±")
    main_topic: str = Field(description="ì˜ˆ: ë°ì´íŠ¸ ë¹„ìš©, ì—°ë½ ë¬¸ì œ, ì´ì„± ë¬¸ì œ ë“±")
    emotion: List[str] = Field(description="ê´€ë ¨ëœ ê°ì • í‚¤ì›Œë“œ, ì˜ˆ: ['ìŠ¤íŠ¸ë ˆìŠ¤', 'ë¶ˆë§Œ', 'ë¶ˆì•ˆ']")

class ContentBody(BaseModel):
    situation_summary: str = Field(description="ì‚¬ì—° ìš”ì•½ (3ë¬¸ì¥ ì´ë‚´)")
    core_conflict: str = Field(description="ê°ˆë“±ì˜ í•µì‹¬ ì›ì¸")
    key_advice: List[str] = Field(description="ìƒë‹´ì‚¬ê°€ ì œì‹œí•œ í•µì‹¬ ì¡°ì–¸ë“¤")
    do_actions: List[str] = Field(alias="do", description="êµ¬ì²´ì ìœ¼ë¡œ í•´ì•¼ í•  í–‰ë™")
    dont_actions: List[str] = Field(alias="dont", description="ì ˆëŒ€ í•˜ì§€ ë§ì•„ì•¼ í•  í–‰ë™")

class ContextMetadata(BaseModel):
    advisor_style: str = Field(description="ìƒë‹´ ìŠ¤íƒ€ì¼ ì˜ˆ: ì§ì„¤, ê³µê°, ë¶„ì„ì ")
    mbti_pair: List[str] = Field(description="ì–¸ê¸‰ëœ ê²½ìš° MBTI ì¡°í•©, ì—†ìœ¼ë©´ ì¶”ë¡ í•˜ê±°ë‚˜ ë¹„ì›Œë‘ ")
    risk_level: str = Field(description="ê´€ê³„ ìœ„í—˜ë„: ë‚®ìŒ, ì¤‘ê°„, ë†’ìŒ, ë§¤ìš° ë†’ìŒ")

class CounselingData(BaseModel):
    retrieval: RetrievalMetadata
    content: ContentBody
    context: ContextMetadata

# ==========================================
# [PART 3] LLM êµ¬ì¡°í™” ë° DB ì ì¬
# ==========================================

def extract_structured_data(raw_transcript: str) -> CounselingData:
    """GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ Raw Textë¥¼ JSON êµ¬ì¡°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    print("ğŸ§  [3/4] ìŠ¤í¬ë¦½íŠ¸ êµ¬ì¡°í™” ë¶„ì„ ì¤‘ (GPT-4o)...")
    completion = client.beta.chat.completions.parse(
        model="gpt-4o-2024-08-06",
        messages=[
            {"role": "system", "content": "ë„ˆëŠ” ì „ë¬¸ì ì¸ ì—°ì•  ìƒë‹´ ë°ì´í„° ë¶„ì„ê°€ì•¼. ì£¼ì–´ì§„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¶„ì„í•´ì„œ JSON í¬ë§·ìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜."},
            {"role": "user", "content": f"ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¶„ì„í•´ì¤˜:\n\n{raw_transcript[:15000]}"}, # í† í° ì œí•œ ê³ ë ¤ (ë„ˆë¬´ ê¸¸ë©´ ìë¦„)
        ],
        response_format=CounselingData,
    )
    return completion.choices[0].message.parsed

def get_embedding(text: str) -> List[float]:
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    response = client.embeddings.create(
        input=text,
        model="text-embedding-3-small"
    )
    return response.data[0].embedding

def upload_to_qdrant(collection_name: str, structured_data: CounselingData):
    """Qdrantì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    print("ğŸ’¾ [4/4] ë²¡í„° DB ì €ì¥ ì¤‘...")
    
    if not qdrant.collection_exists(collection_name):
        qdrant.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=1536, distance=Distance.COSINE),
        )

    # ê²€ìƒ‰ ìµœì í™”ìš© í…ìŠ¤íŠ¸ ìƒì„±
    embedding_source_text = f"""
    ì£¼ì œ: {structured_data.retrieval.main_topic}
    ë‹¨ê³„: {structured_data.retrieval.relationship_stage}
    ê°ì •: {", ".join(structured_data.retrieval.emotion)}
    ìƒí™©: {structured_data.content.situation_summary}
    ê°ˆë“±: {structured_data.content.core_conflict}
    """
    
    vector = get_embedding(embedding_source_text)
    
    point_id = str(uuid.uuid4())
    payload_dict = structured_data.model_dump(by_alias=True)
    
    # ì›ë³¸ ì¶œì²˜ ì¶”ì ì„ ìœ„í•´ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ì„ íƒì‚¬í•­)
    # payload_dict["source_url"] = VIDEO_URL 

    qdrant.upsert(
        collection_name=collection_name,
        points=[PointStruct(id=point_id, vector=vector, payload=payload_dict)]
    )
    print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ! ID: {point_id}")

# ==========================================
# [MAIN] ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
# ==========================================

if __name__ == "__main__":
    # 1. FFmpeg ì—”ì§„ì´ ë“¤ì–´ìˆëŠ” í´ë” ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”.
    # ì˜ˆ: Cë“œë¼ì´ë¸Œ ë°”ë¡œ ì•„ë˜ ffmpeg í´ë”ë¥¼ ë§Œë“œì…¨ë‹¤ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.
    FFMPEG_PATH = r"C:\ffmpeg\bin" 

    # 2. ì‹œìŠ¤í…œ í™˜ê²½ ë³€ìˆ˜(PATH)ì— ì´ ê²½ë¡œë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ Whisperê°€ ë‚´ë¶€ì ìœ¼ë¡œ ffprobeë¥¼ ì°¾ì„ ë•Œ ì´ í´ë”ë¥¼ ë’¤ì§€ê²Œ ë©ë‹ˆë‹¤.
    os.environ["PATH"] = FFMPEG_PATH + os.pathsep + os.environ["PATH"]
    
    # 3. ë¶„ì„í•  ìœ íŠœë¸Œ ì£¼ì†Œ
    TARGET_URL = "https://www.youtube.com/watch?v=7Q9MhbE8WFg" 
    
    # 4. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    audio_file = download_audio_from_youtube(TARGET_URL)
    
    if audio_file and os.path.exists(audio_file):
        try:
            # ì´ì œ WinError 2 ì—†ì´ í†µê³¼í•©ë‹ˆë‹¤!
            raw_script = transcribe_with_local_whisper(audio_file)
            
            if raw_script:
                print(f"âœ… ì¶”ì¶œ ì„±ê³µ! í…ìŠ¤íŠ¸ ê¸¸ì´: {len(raw_script)}")
                structured_data = extract_structured_data(raw_script)
                upload_to_qdrant("love_counseling_db", structured_data)
        finally:
            if os.path.exists(audio_file):
                os.remove(audio_file)
    else:
        print("âŒ ì˜¤ë””ì˜¤ íŒŒì¼ ì¤€ë¹„ ì‹¤íŒ¨")