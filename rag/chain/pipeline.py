# LLM Model and LangChain pipeline module
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import (
    RunnablePassthrough,
    RunnableParallel,
    RunnableLambda,
)
from langchain_core.prompts import ChatPromptTemplate
from rag.config import Config

def init_llm():
    """
    Config ì„¤ì •ì„ ë°”íƒ•ìœ¼ë¡œ LLM ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
    """
    return ChatOpenAI(
        model=Config.MODEL_NAME,
        temperature=Config.TEMPERATURE,
        max_tokens=Config.MAX_TOKENS,
        openai_api_key=Config.OPENAI_API_KEY
    )

def format_docs(docs):
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìžì—´ë¡œ ê²°í•©í•©ë‹ˆë‹¤.
    """
    return "\n\n".join(doc.page_content for doc in docs)
    
def rewrite_query(original_query):
    """
    ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ê²€ìƒ‰ì— ìµœì í™”ëœ í˜•íƒœë¡œ ìž¬ìž‘ì„±í•©ë‹ˆë‹¤.
    """
    llm = ChatOpenAI(model="gpt-4o-mini", openai_api_key=Config.OPENAI_API_KEY)

    # ðŸ“ ì—°ì•  ìƒë‹´ ë°ì´í„°ì…‹ì˜ íŠ¹ì„±ì— ë§žì¶˜ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì§ˆë¬¸ ìž¬ìž‘ì„± ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ì§ˆë¬¸ì„ ê²€ìƒ‰ ì—”ì§„ì´ ì—°ì•  ìƒë‹´ ì‚¬ë¡€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ê°€ìž¥ ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ìž˜ ì°¾ì„ ìˆ˜ ìžˆë„ë¡ ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•œ ë¬¸ìž¥ìœ¼ë¡œ í•œ ì¤„ë§Œ ìž¬ìž‘ì„±í•˜ì„¸ìš”."),
        ("human", f"ì›ëž˜ ì§ˆë¬¸: {original_query}")
    ])
    
    chain = prompt | llm
    rewritten_query = chain.invoke({}).content
    print(f"ðŸ”„ ìž¬ìž‘ì„±ëœ ì§ˆë¬¸: {rewritten_query}") # ë””ë²„ê¹…ìš©
    return rewritten_query


def create_chain(llm, retriever, prompt):
    """
    retrieverì™€ promptë¥¼ ì£¼ìž…ë°›ì•„
    LCELì„ ì´ìš©í•œ RAG íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.
    """
    # LangSmithì—ì„œ ë‹¨ê³„ë³„ë¡œ ì‹ë³„í•˜ê¸° ìœ„í•´ run_name ì§€ì •
    llm = llm.with_config({"run_name": "chat_model"})

    # 1. Contextì™€ Questionì„ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬
    setup_and_retrieval = RunnableParallel(
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
    ).with_config({"run_name": "retrieve_and_prepare"})

    # 2. ì „ì²´ ì²´ì¸ êµ¬ì„±: Retrieval -> Prompt -> LLM -> OutputParser
    chain = (
        RunnableLambda(rewrite_query).with_config({"run_name": "rewrite_query"})
        | setup_and_retrieval
        | prompt
        | llm
        | StrOutputParser()
    ).with_config({"run_name": "love_counseling_rag"})
    
    return chain

