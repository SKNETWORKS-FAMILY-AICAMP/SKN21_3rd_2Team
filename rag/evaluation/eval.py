import os
import pandas as pd
from dotenv import load_dotenv
from datasets import Dataset

from ragas import evaluate
from ragas.metrics import answer_relevancy

from langchain_openai import ChatOpenAI

# ===============================
# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
# ===============================
load_dotenv()

# ===============================
# í‰ê°€ ì‹¤í–‰ í•¨ìˆ˜
# ===============================
def run_evaluation():
    test_questions = [
        "ì—¬ìì¹œêµ¬ì™€ ì—°ë½ ë¬¸ì œë¡œ ìì£¼ ì‹¸ì›Œ. ë‚´ê°€ ë„ˆë¬´ ì§‘ì°©í•˜ëŠ” ê±¸ê¹Œ?",
        "ì¸ íƒ€ëŠ” ì‚¬ëŒì´ ì¹´í†¡ ë‹µì¥ì´ ë„ˆë¬´ ëŠë ¤. ì´ê±° ê·¸ë¦°ë¼ì´íŠ¸ ë§ì•„?",
    ]

    results = {
        "question": [],
        "answer_no_prompt": [],
        "answer_with_prompt": [],
        "relevancy_no_prompt": [],
        "relevancy_with_prompt": []
    }

    # Chat LLM (invoke ë°©ì‹ ì‚¬ìš©)
    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0
    )

    for q in test_questions:
        print(f"\nğŸ“ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘: {q}")

        # ======================================
        # 1. Prompt / RAG ì—†ì´ ë‹µë³€ ìƒì„±
        # ======================================
        try:
            response = llm.invoke(q)
            answer_no_prompt = response.content
            print(f"ğŸ”¹ Answer without prompt:\n{answer_no_prompt}")
        except Exception as e:
            print(f"âŒ Error generating answer without prompt: {e}")
            answer_no_prompt = ""

        # ======================================
        # 2. Prompt + RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
        # ======================================
        try:
            from retrieve_test import get_rag_response
            rag_response = get_rag_response(q, prompt_file="prompt.md")
            answer_with_prompt = rag_response.get("answer", "")
            print(f"ğŸ”¹ Answer with prompt:\n{answer_with_prompt}")
        except Exception as e:
            print(f"âŒ Error generating answer with prompt: {e}")
            answer_with_prompt = ""

        # ======================================
        # 3. Ragas answer_relevancy í‰ê°€
        # ======================================
        try:
            dataset_no_prompt = Dataset.from_dict({
                "question": [q],
                "answer": [answer_no_prompt],
            })

            score_no_prompt = evaluate(
                dataset=dataset_no_prompt,
                metrics=[answer_relevancy],
                llm=llm
            )

            relevancy_no_prompt = score_no_prompt["answer_relevancy"]
        except Exception as e:
            print(f"âŒ Error calculating relevancy (no prompt): {e}")
            relevancy_no_prompt = None

        try:
            dataset_with_prompt = Dataset.from_dict({
                "question": [q],
                "answer": [answer_with_prompt],
            })

            score_with_prompt = evaluate(
                dataset=dataset_with_prompt,
                metrics=[answer_relevancy],
                llm=llm
            )

            relevancy_with_prompt = score_with_prompt["answer_relevancy"]
        except Exception as e:
            print(f"âŒ Error calculating relevancy (with prompt): {e}")
            relevancy_with_prompt = None

        # ======================================
        # 4. ê²°ê³¼ ì €ì¥
        # ======================================
        results["question"].append(q)
        results["answer_no_prompt"].append(answer_no_prompt)
        results["answer_with_prompt"].append(answer_with_prompt)
        results["relevancy_no_prompt"].append(relevancy_no_prompt)
        results["relevancy_with_prompt"].append(relevancy_with_prompt)

    # ======================================
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    # ======================================
    df = pd.DataFrame(results)
    output_file = "answer_relevancy_comparison.csv"
    df.to_csv(output_file, index=False, encoding="utf-8-sig")

    print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ê°€ '{output_file}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    run_evaluation()
