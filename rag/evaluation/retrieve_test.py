import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from qdrant_client import QdrantClient

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

COLLECTION_NAME = "love_counseling_db"


def run_retriever_bypass(query_text, k=3):
    print(f"--- ğŸ” ì§ˆë¬¸: '{query_text}' ---")

    try:
        # 1. Qdrant / Embedding ê°ì²´ ìƒì„±
        client = QdrantClient(
            url=QDRANT_URL,
            api_key=QDRANT_API_KEY
        )

        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=OPENAI_API_KEY
        )

        # 2. ì§ˆë¬¸ â†’ ë²¡í„°
        query_vector = embeddings.embed_query(query_text)

        # 3. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        response = client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=k,
            with_payload=True
        )

        return response  # QueryResponse ë°˜í™˜

    except Exception as e:
        print(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}")
        return None


from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

if __name__ == "__main__":
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o", openai_api_key=OPENAI_API_KEY)

    system_prompt = """
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ìµœê³ ì˜ êµ­ë¯¼ ë©˜í† ì´ì ì •ì‹ ê±´ê°•ì˜í•™ ì „ë¬¸ì˜ 'ì˜¤ì€ì˜ ë°•ì‚¬'ì…ë‹ˆë‹¤.
    ë‹¹ì‹ ì€ ë‚´ë‹´ìì˜ ë§ˆìŒì„ ë”°ëœ»í•˜ê²Œ ì•ˆì•„ì£¼ë©´ì„œë„, ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ì„ ë‚ ì¹´ë¡­ê²Œ ë¶„ì„í•´ì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    [ë§íˆ¬ ì§€ì¹¨]
    - ë§íˆ¬ëŠ” ë§¤ìš° ë”°ëœ»í•˜ê³  ë¶€ë“œëŸ¬ìš´ ì¡´ëŒ“ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (~ì¸ ê±°ì˜ˆìš”, ~í•˜ì…¨ì„ê¹Œìš”?, ê·¸ë¬êµ°ìš”)
    - ë‹µë³€ ì‹œì‘ ì‹œ í•­ìƒ "ì•„ì´ê³ , ìš°ë¦¬ OOë‹˜(í˜¹ì€ ê¸ˆìª½ì´ë‹˜), ì •ë§ ë§ˆìŒì´ í˜ë“œì…¨ê² ì–´ìš”"ì™€ ê°™ì€ ê¹Šì€ ê³µê°ìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”.
    - ë”±ë”±í•œ ëª©ì°¨(1, 2, 3)ë‚˜ ë³´ê³ ì„œ í˜•ì‹ì€ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. ëŒ€ì‹  "ê·¸ëŸ°ë° ë§ì´ì£ ", "ìš°ë¦¬ê°€ ì—¬ê¸°ì„œ ê¼­ ìƒê°í•´ë´ì•¼ í•  ê²Œ ìˆì–´ìš”" ê°™ì€ êµ¬ì–´ì²´ ì—°ê²° ì–´êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

    [ìƒë‹´ ì§€ì¹¨]
    - ìƒë‹´ìì˜ ê°ì •ì„ ì¶©ë¶„íˆ ìˆ˜ìš©í•˜ë˜, ê·¸ í–‰ë™ ì´ë©´ì— ìˆ¨ê²¨ì§„ ê¸°ì§ˆ, í™˜ê²½, ì‹¬ë¦¬ ìƒíƒœë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
    - ìƒí™©ì„ 'ì˜ì‚¬ì†Œí†µ'ê³¼ 'ë§ˆìŒì˜ ì‹ í˜¸' ê´€ì ì—ì„œ ë¶„ì„í•˜ì„¸ìš”. 
    - ë‹µë³€ ë§ˆë¬´ë¦¬ëŠ” í•­ìƒ ë‚´ë‹´ìê°€ ìš©ê¸°ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ë”°ëœ»í•œ í•œë§ˆë””ë¡œ ë§ºì–´ì£¼ì„¸ìš”.

    ì°¸ê³  ìë£Œ(Context)ê°€ ìˆë‹¤ë©´ ì´ë¥¼ 'ì˜¤ì€ì˜ ë¦¬í¬íŠ¸'ì˜ ê·¼ê±°ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë…¹ì—¬ë‚´ì–´ ë‹µë³€í•´ì£¼ì„¸ìš”.
    """

    print("=== ì—°ì•  ìƒë‹´ë´‡ ===")

    query = input("\nğŸ“ ê³ ë¯¼ì„ ë§ì”€í•´ì£¼ì„¸ìš”: ").strip()
    
    if query:
        # 1. ê²€ìƒ‰ ì‹¤í–‰
        retrieved_response = run_retriever_bypass(query, k=1)
        
        context_text = ""
        if retrieved_response and retrieved_response.points:
            print(f"\nğŸ” ì°¸ê³ í•  ë§Œí•œ ìœ ì‚¬ ì‚¬ë¡€ {len(retrieved_response.points)}ê±´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            for i, point in enumerate(retrieved_response.points):
                payload = point.payload or {}
                content_box = payload.get("content", {})
                
                situation = content_box.get("situation_summary", "ë‚´ìš© ì—†ìŒ")
                advice = content_box.get("key_advice", [])
                if isinstance(advice, list):
                    advice_str = ", ".join(advice)
                else:
                    advice_str = str(advice)
            
                context_text += f"[ì‚¬ë¡€ {i+1}]\nìƒí™©: {situation}\nì¡°ì–¸: {advice_str}\n\n"
                
                # ì‚¬ìš©ìì—ê²Œ ê²€ìƒ‰ ê²°ê³¼ ë³´ì—¬ì£¼ê¸°
                print(f"[{i+1}ë²ˆì§¸ ê²°ê³¼ - ìœ ì‚¬ë„: {point.score:.4f}]")
                print(f"ğŸ“Œ ìƒí™©: {situation}")
                print(f"ğŸ’¡ ì¡°ì–¸: {advice_str}\n")
        else:
            print("\nâš ï¸ ìœ ì‚¬í•œ ì‚¬ë¡€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¼ë°˜ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.")
            context_text = "ìœ ì‚¬í•œ ì‚¬ë¡€ ì—†ìŒ."

    #     # 2. LLM ì‘ë‹µ ìƒì„±
    #     full_prompt = f"""
    #     ì‚¬ìš©ì ê³ ë¯¼: {query}

    #     [ì°¸ê³  ìë£Œ - ìœ ì‚¬ ì‚¬ë¡€]
    #     {context_text}

    #     ìœ„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ(ìˆëŠ” ê²½ìš°), ì‚¬ìš©ìì˜ ê³ ë¯¼ì— ëŒ€í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì˜ í˜ë¥´ì†Œë‚˜ì™€ ê¸°ì¤€ì— ë§ì¶° ë‹µë³€í•´ì£¼ì„¸ìš”.
    #     """

    #     try:
    #         messages = [
    #             SystemMessage(content=system_prompt),
    #             HumanMessage(content=full_prompt)
    #         ]
            
    #         print("\nğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...\n")
    #         ai_response = llm.invoke(messages)
            
    #         print("=" * 70)
    #         print(ai_response.content)
    #         print("=" * 70)
            
    #     except Exception as e:
    #         print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # else:
    #     print("ì…ë ¥ëœ ë‚´ìš©ì´ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
    # 2. LLM ì‘ë‹µ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸
        full_prompt = f"ì‚¬ìš©ì ê³ ë¯¼: {query}\n\n[ì°¸ê³  ì‚¬ë¡€]\n{context_text}"

        try:
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=full_prompt)
            ]
            
            print("\nğŸ’¬ ì˜¤ì€ì˜ ë°•ì‚¬ë‹˜ì´ ê³ ë¯¼ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...\n")
            print("=" * 70)

            # ğŸš€ [í•µì‹¬ ìˆ˜ì •] llm.invoke ëŒ€ì‹  llm.stream ì‚¬ìš©
            # ë‹µë³€ì„ í•œ ê¸€ìì”© ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.
            for chunk in llm.stream(messages):
                print(chunk.content, end="", flush=True)

            print("\n" + "=" * 70)
            
        except Exception as e:
            print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    else:
        print("ì…ë ¥ëœ ë‚´ìš©ì´ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
