import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from qdrant_client import QdrantClient
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

COLLECTION_NAME = "love_counseling_db"


def run_retriever_bypass(query_text, k=3):
    print(f"--- ğŸ” ì§ˆë¬¸: '{query_text}' ---")

    try:
        # 1. Qdrant / Embedding ê°ì²´ ìƒì„±
        client = QdrantClient(
            url=QDRANT_URL,
            api_key=QDRANT_API_KEY
        )

        embeddings = OpenAIEmbeddings(
            model="text-embedding-3-small",
            openai_api_key=OPENAI_API_KEY
        )

        # 2. ì§ˆë¬¸ â†’ ë²¡í„°
        query_vector = embeddings.embed_query(query_text)

        # 3. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
        response = client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=k,
            with_payload=True
        )

        return response  # QueryResponse ë°˜í™˜

    except Exception as e:
        print(f"ğŸ”¥ ì—ëŸ¬ ë°œìƒ: {e}")
        return None


def get_rag_response(query, prompt_file="promt.md"):
    """
    RAG íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•˜ì—¬ ë‹µë³€ê³¼ ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o", openai_api_key=OPENAI_API_KEY)

    # í”„ë¡¬í”„íŠ¸ íŒŒì¼ ì½ê¸°
    prompt_path = os.path.join(os.path.dirname(__file__), prompt_file)
    try:
        with open(prompt_path, "r", encoding="utf-8") as f:
            system_prompt = f.read()
    except FileNotFoundError:
        print(f"âš ï¸ í”„ë¡¬í”„íŠ¸ íŒŒì¼({prompt_path})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        system_prompt = "ë‹¹ì‹ ì€ ì—°ì•  ìƒë‹´ê°€ì…ë‹ˆë‹¤."

    # 1. ê²€ìƒ‰ ì‹¤í–‰
    retrieved_response = run_retriever_bypass(query, k=1)
    
    context_text = ""
    retrieved_contexts = []
    
    if retrieved_response and retrieved_response.points:
        for i, point in enumerate(retrieved_response.points):
            payload = point.payload or {}
            content_box = payload.get("content", {})
            
            situation = content_box.get("situation_summary", "ë‚´ìš© ì—†ìŒ")
            advice = content_box.get("key_advice", [])
            if isinstance(advice, list):
                advice_str = ", ".join(advice)
            else:
                advice_str = str(advice)
        
            context_text += f"[ì‚¬ë¡€ {i+1}]\nìƒí™©: {situation}\nì¡°ì–¸: {advice_str}\n\n"
            retrieved_contexts.append(f"ìƒí™©: {situation}, ì¡°ì–¸: {advice_str}")
    else:
        context_text = "ìœ ì‚¬í•œ ì‚¬ë¡€ ì—†ìŒ."

    # 2. LLM ì‘ë‹µ ìƒì„±
    full_prompt = f"""
    ì‚¬ìš©ì ê³ ë¯¼: {query}

    [ì°¸ê³  ìë£Œ - ìœ ì‚¬ ì‚¬ë¡€]
    {context_text}

    ìœ„ ì°¸ê³  ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ(ìˆëŠ” ê²½ìš°), ì‚¬ìš©ìì˜ ê³ ë¯¼ì— ëŒ€í•´ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì˜ í˜ë¥´ì†Œë‚˜ì™€ ê¸°ì¤€ì— ë§ì¶° ë‹µë³€í•´ì£¼ì„¸ìš”.
    """

    try:
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=full_prompt)
        ]
        
        ai_response = llm.invoke(messages)
        return {
            "query": query,
            "answer": ai_response.content,
            "contexts": retrieved_contexts
        }
        
    except Exception as e:
        print(f"âŒ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None


if __name__ == "__main__":
    print("=== ì—°ì•  ìƒë‹´ë´‡ ===")
    print("ğŸ“ ê³ ë¯¼ì„ ë§ì”€í•´ì£¼ì„¸ìš” (ì…ë ¥ì„ ì™„ë£Œí•˜ë ¤ë©´ ë‚´ìš© ì…ë ¥ í›„ ì—”í„°ë¥¼ í•œ ë²ˆ ë” ëˆ„ë¥´ì„¸ìš”):")

    lines = []
    while True:
        try:
            line = input()
            if not line:
                break
            lines.append(line)
        except EOFError:
            break
    
    query = "\n".join(lines).strip()
    
    if query:
        response = get_rag_response(query, "promt.md")
        
        if response:
            print(f"\nğŸ” ì°¸ê³ í•  ë§Œí•œ ìœ ì‚¬ ì‚¬ë¡€ {len(response['contexts'])}ê±´ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            # ìƒì„¸ ì¶œë ¥ì€ í•¨ìˆ˜ ë‚´ë¶€ê°€ ì•„ë‹Œ ì—¬ê¸°ì„œ context_textë¥¼ ì¬êµ¬ì„±í•˜ê±°ë‚˜ responseì— í¬í•¨í•´ì•¼ í•˜ì§€ë§Œ
            # ê¸°ì¡´ ì¶œë ¥ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ ê°„ë‹¨íˆ ì²˜ë¦¬í•˜ê±°ë‚˜ í•¨ìˆ˜ì—ì„œ printë¥¼ í•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
            for i, ctx in enumerate(response['contexts']):
                 print(f"[{i+1}] {ctx}")

            print("\nğŸ’¬ ë‹µë³€ ìƒì„± ì™„ë£Œ\n")
            print("=" * 70)
            print(response['answer'])
            print("=" * 70)

    else:
        print("ì…ë ¥ëœ ë‚´ìš©ì´ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
